{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Systematically-Download-and-Consolidate-Data-Files-from-a-Website-with-Python\" data-toc-modified-id=\"Systematically-Download-and-Consolidate-Data-Files-from-a-Website-with-Python-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Systematically Download and Consolidate Data Files from a Website with Python</a></span></li><li><span><a href=\"#Part-1:-Web-scrape-all-download-links-from-Bixi-website\" data-toc-modified-id=\"Part-1:-Web-scrape-all-download-links-from-Bixi-website-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Part 1: Web-scrape all download links from Bixi website</a></span></li><li><span><a href=\"#Part-2:-Downloading-the-files-from-each-link\" data-toc-modified-id=\"Part-2:-Downloading-the-files-from-each-link-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Part 2: Downloading the files from each link</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Systematically Download and Consolidate Data Files from a Website with Python"
=======
    "# Web Scraping - Downloading CSV's\n",
    "\n",
    "By: Andrew Yew <br>\n",
    "Last updated: 2023-05-17"
>>>>>>> parent of af2591a (added some of part 2)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "Often times when collecting data for a project, we encounter consumer facing websites that do not have full Application Programming Interfaces (APIs) that allow for data to be accessed and downloaded in a programatic way.\n",
    "What this usually means is that we as data scientists have to manually click the download buttons on a website and provide a directory for the file to be saved on a local drive.\n",
    "\n",
    "For one or two files this is maneagable but as companies continue to publish more open data annually, it is not unusual to find websites such as the one shown below belonging to [Bixi](https://bixi.com/en/open-data), a bike-share company in the city of Montreal, accessed 2023-May-17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bixi](images/001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above, we see a file for each year, with the most recent year having a file for each month. Downloading the data manually would mean having to repeatedly click download 16 times, not including the time for renaming and consolidating the files later into a data folder.\n",
    "\n",
    "In this notebook, we will explore a more systematic way to access and download all data links within a website using requests and BeautifulSoup in a Jupyter Notebook running Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is split into two parts:\n",
    "- part 1: how to web-scrape all download links from the Bixi website\n",
    "- part 2: using the web-scraped links, how to download and store the data as a Comma Separated Value (CSV) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web-scrape all download links from Bixi website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with importing required packages. Note that for readability, the required packages will be imported as the need arises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests # for scraping data from websites\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing any web-scraping, the first step is to identify the website(s) from which to scrape from. For this notebook, the website where Bixi hosts its data was identified and stored in the \"url\" variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bixi.com/en/open-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gather data from a website, we will be using the get method within the requests package. Further documentation on requests is available [here](https://docs.python-requests.org/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to gather a response\n",
    "response = requests.get(url = url, allow_redirects =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we examine the status code of the response to determine if it was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code: 200, status: OK\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response status code: {response.status_code}, status: {response.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Next, we can use the \"text\" method on the response object to visually examine the contents of the response. It was expected to receive a large text blob containing all the elements of the provided website. To avoid the notebook from becoming too long, only the first 100 characters of the response were displayed below."
=======
    "Then, we can use the \"text\" method on the response object to visually examine the contents of the response. It was expected to receive a large text blob containing all the elements of the provided website. To avoid the notebook from becoming too long, only the first 500 characters of the response were displayed below."
>>>>>>> parent of af2591a (added some of part 2)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html>\\n<html class=\"no-js\" lang=\"en\" data-scrollbar>\\n\\t<head>\\n\\t\\t<meta charset=\"utf-8\">\\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"/>\\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"/>\\n\\n<link rel=\"apple-touch-icon\" sizes=\"57x57\" href=\"/assets/favicon/apple-touch-icon-57x57.png\">\\n<link rel=\"apple-touch-icon\" sizes=\"60x60\" href=\"/assets/favicon/apple-touch-icon-60x60.png\">\\n<link rel=\"apple-touch-icon\" sizes=\"72x72\" href=\"/assets/favicon/apple-touch-icon-72x72.png\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data scientists, most of the contents within response are not useful as they pertain to the design and layout of the website. How then do we identify the specific components that contain the data we are seeking to scrape?\n",
    "\n",
    "To answer this, we may perform an element inspection on the specific desired part of a website using a web browser. Right click on the element containing the data (\"Year 2021\" below), then select \"Inspect\".   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bixi](images/002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the element inspector that appears, the element and its corresponding code block was highlighted. In the code block, the url embedded within the element can be identified after the \"a\" html tag. Clicking on this url will lead to the download of a zip file containing the data. \n",
    "\n",
    "The other data containing urls were observed to be above and below the highlighted code block. For this case, note that all the other data urls contain the string 'amazonaws' as a common pattern. This means that the data is actually stored on an Amazon S3 bucket. To facilitate the web-scraping of all urls that contain data, the string pattern 'amazonaws' will be used to identify such urls from the response object downloaded earlier. \n",
    "\n",
    "Side note, although there is another Python package that specializes in dealing with Amazon S3 buckets, the method presented in this notebook is more generizable to other data stored outside of Amazon S3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bixi](images/003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the string pattern that allows us to identify urls containing the data within the response, the next step is to use the BeautifulSoup package to turn the large blob of response text into structured soup object for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the string pattern as a variable\n",
    "url_string_pattern = 'amazonaws'\n",
    "\n",
    "# Convert response text blob into structured HTML format\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check type of soup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the soup object is structured HTML tags, the HTML tag 'a' can be used to identify all urls within the soup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all tags\n",
    "url_tags = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, the HTML tag 'a' along with any urls have been extracted into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"logo\" href=\"/en\"></a>,\n",
       " <a class=\"altLang\" href=\"https://www.bixi.com/fr/donnees-ouvertes\">Fran√ßais</a>,\n",
       " <a href=\"https://www.bixi.com/en/network-info\">Network info</a>,\n",
       " <a href=\"https://www.bixi.com/en/contact-us\">Contact us</a>,\n",
       " <a class=\"icon-facebook social\" href=\"https://www.facebook.com/BIXImontreal/\" target=\"_blank\"></a>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visually examine first 5 tags\n",
    "url_tags[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "print(type(url_tags[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the url within each tag must be extracted. A for loop was combined with the get method for each tag object. Within the get method, 'href' was used to  was used to identify the urls within each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a blank list to store extracted url\n",
    "url_list = list()\n",
    "\n",
    "# Loop through each tag to extract urls\n",
    "for tag in url_tags:\n",
    "    url_list.append(tag.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/en',\n",
       " 'https://www.bixi.com/fr/donnees-ouvertes',\n",
       " 'https://www.bixi.com/en/network-info',\n",
       " 'https://www.bixi.com/en/contact-us',\n",
       " 'https://www.facebook.com/BIXImontreal/']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visual examination of the urls extracted from tags\n",
    "url_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last step is to filter the list for only the urls that contain the desired data using the defined search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use pandas str.contains() method, the list of extracted urls was first converted into a DataFrame\n",
    "url_df = pd.DataFrame(url_list, columns = ['extracted_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.bixi.com/fr/donnees-ouvertes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.bixi.com/en/network-info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.bixi.com/en/contact-us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.facebook.com/BIXImontreal/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              extracted_url\n",
       "0                                       /en\n",
       "1  https://www.bixi.com/fr/donnees-ouvertes\n",
       "2      https://www.bixi.com/en/network-info\n",
       "3        https://www.bixi.com/en/contact-us\n",
       "4    https://www.facebook.com/BIXImontreal/"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the extracted urls\n",
    "url_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any null values since null values would mean no urls\n",
    "url_df.dropna(inplace = True)\n",
    "\n",
    "# Define filter condition to keep only urls that contain the string pattern\n",
    "cond1 = url_df['extracted_url'].str.lower().str.contains(url_string_pattern)\n",
    "\n",
    "# Use the defined condition to filter the extracted url list\n",
    "url_df = url_df.loc[cond1].reset_index(drop = True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, from the web-scraped HTML, we have identified and gathered all 16 urls within the BIXI website that lead to data download without having to visually identify and click on each link within the BIXI website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of urls extracted is 16\n",
      "Visually examine first 5 filtered urls\n",
<<<<<<< HEAD
      "URL 1 / 16 : https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontrealrentals2014-f040e0.zip\n",
      "URL 2 / 16 : https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontrealrentals2015-69fdf0.zip\n",
      "URL 3 / 16 : https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontrealrentals2016-912f00.zip\n",
      "URL 4 / 16 : https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontrealrentals2017-d4d086.zip\n",
      "URL 5 / 16 : https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontrealrentals2018-96034e.zip\n"
=======
      "https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontrealrentals2014-f040e0.zip\n",
      "https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontrealrentals2015-69fdf0.zip\n",
      "https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontrealrentals2016-912f00.zip\n",
      "https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontrealrentals2017-d4d086.zip\n",
      "https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontrealrentals2018-96034e.zip\n"
>>>>>>> parent of af2591a (added some of part 2)
     ]
    }
   ],
   "source": [
    "print(f\"The number of urls extracted is {url_df.shape[0]}\")\n",
    "\n",
    "# Visually examining first 5\n",
    "print(\"Visually examine first 5 filtered urls\")\n",
    "for url in url_df['extracted_url'].to_list()[0:5]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Downloading the files from each link"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https:</td>\n",
       "      <td></td>\n",
       "      <td>sitewebbixi.s3.amazonaws.com</td>\n",
       "      <td>uploads</td>\n",
       "      <td>docs</td>\n",
       "      <td>biximontrealrentals2014-f040e0.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https:</td>\n",
       "      <td></td>\n",
       "      <td>sitewebbixi.s3.amazonaws.com</td>\n",
       "      <td>uploads</td>\n",
       "      <td>docs</td>\n",
       "      <td>biximontrealrentals2015-69fdf0.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https:</td>\n",
       "      <td></td>\n",
       "      <td>sitewebbixi.s3.amazonaws.com</td>\n",
       "      <td>uploads</td>\n",
       "      <td>docs</td>\n",
       "      <td>biximontrealrentals2016-912f00.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https:</td>\n",
       "      <td></td>\n",
       "      <td>sitewebbixi.s3.amazonaws.com</td>\n",
       "      <td>uploads</td>\n",
       "      <td>docs</td>\n",
       "      <td>biximontrealrentals2017-d4d086.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https:</td>\n",
       "      <td></td>\n",
       "      <td>sitewebbixi.s3.amazonaws.com</td>\n",
       "      <td>uploads</td>\n",
       "      <td>docs</td>\n",
       "      <td>biximontrealrentals2018-96034e.zip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0 1                             2        3     4  \\\n",
       "0  https:    sitewebbixi.s3.amazonaws.com  uploads  docs   \n",
       "1  https:    sitewebbixi.s3.amazonaws.com  uploads  docs   \n",
       "2  https:    sitewebbixi.s3.amazonaws.com  uploads  docs   \n",
       "3  https:    sitewebbixi.s3.amazonaws.com  uploads  docs   \n",
       "4  https:    sitewebbixi.s3.amazonaws.com  uploads  docs   \n",
       "\n",
       "                                    5  \n",
       "0  biximontrealrentals2014-f040e0.zip  \n",
       "1  biximontrealrentals2015-69fdf0.zip  \n",
       "2  biximontrealrentals2016-912f00.zip  \n",
       "3  biximontrealrentals2017-d4d086.zip  \n",
       "4  biximontrealrentals2018-96034e.zip  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using the string split method, split the url into separate chunks\n",
    "split_df = url_df['extracted_url'].str.lower().str.split('/', expand = True)\n",
    "\n",
    "# Visually examine the first 5 rows\n",
    "display(split_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file name for each url can be found in the last column of the split url. Thus, the last column of the split_df can be added to url_df as the file names for each url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_url</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://sitewebbixi.s3.amazonaws.com/uploads/d...</td>\n",
       "      <td>biximontrealrentals2014-f040e0.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://sitewebbixi.s3.amazonaws.com/uploads/d...</td>\n",
       "      <td>biximontrealrentals2015-69fdf0.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://sitewebbixi.s3.amazonaws.com/uploads/d...</td>\n",
       "      <td>biximontrealrentals2016-912f00.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://sitewebbixi.s3.amazonaws.com/uploads/d...</td>\n",
       "      <td>biximontrealrentals2017-d4d086.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://sitewebbixi.s3.amazonaws.com/uploads/d...</td>\n",
       "      <td>biximontrealrentals2018-96034e.zip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       extracted_url  \\\n",
       "0  https://sitewebbixi.s3.amazonaws.com/uploads/d...   \n",
       "1  https://sitewebbixi.s3.amazonaws.com/uploads/d...   \n",
       "2  https://sitewebbixi.s3.amazonaws.com/uploads/d...   \n",
       "3  https://sitewebbixi.s3.amazonaws.com/uploads/d...   \n",
       "4  https://sitewebbixi.s3.amazonaws.com/uploads/d...   \n",
       "\n",
       "                            file_name  \n",
       "0  biximontrealrentals2014-f040e0.zip  \n",
       "1  biximontrealrentals2015-69fdf0.zip  \n",
       "2  biximontrealrentals2016-912f00.zip  \n",
       "3  biximontrealrentals2017-d4d086.zip  \n",
       "4  biximontrealrentals2018-96034e.zip  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accessing the last column in the split_df and adding it to url_df\n",
    "url_df['file_name'] = split_df.iloc[:,-1]\n",
    "\n",
    "# Visually examine the first 5 rows\n",
    "display(url_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data download urls and the file names to save each download as have been defined, the next step is to use the [urllib](https://docs.python.org/3/library/urllib.html) and [shutil](https://docs.python.org/3/library/shutil.html) packages to perform the downloads. The urllib package is different from the earlier used requests package as the latter is focused on human readable HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib # URL handling modules\n",
    "import shutil # High level operation on files (example copying and removing)\n",
    "import time # For timing and measuring progress of download\n",
    "import numpy as np # For rounding digits\n",
    "import datetime #For measuring time\n",
    "import pytz #For defining timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each data download url in url_df, use urllib to gather the response (the zip data file) from the url, then use shutil to save the response into the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading data file 16 of 16, time taken: 1.725 seconds.\r"
=======
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot mask with non-boolean array containing NA / NaN values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hc/dd4qvphd7j59ktfp1751r_4h0000gn/T/ipykernel_3620/2018266809.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murl_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'extracted_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amazonaws'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/streamlit/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3447\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3448\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/streamlit/lib/python3.7/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0;31m# Don't raise on e.g. [\"A\", \"B\", np.nan], see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0;31m#  test_loc_getitem_list_of_labels_categoricalindex_with_na\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot mask with non-boolean array containing NA / NaN values"
>>>>>>> parent of af2591a (added some of part 2)
     ]
    }
   ],
   "source": [
    "url_df[url_df['extracted_url'].str.contains('amazonaws')]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average download time is 4.077 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_url</th>\n",
       "      <th>file_name</th>\n",
       "      <th>date_accessed</th>\n",
       "      <th>time_taken_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://sitewebbixi.s3.amazonaws.com/uploads/d...</td>\n",
       "      <td>biximontrealrentals2014-f040e0.zip</td>\n",
       "      <td>2023-05-26 13:42:35.764567+00:00</td>\n",
       "      <td>3.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://sitewebbixi.s3.amazonaws.com/uploads/d...</td>\n",
       "      <td>biximontrealrentals2015-69fdf0.zip</td>\n",
       "      <td>2023-05-26 13:42:38.769303+00:00</td>\n",
       "      <td>2.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://sitewebbixi.s3.amazonaws.com/uploads/d...</td>\n",
       "      <td>biximontrealrentals2016-912f00.zip</td>\n",
       "      <td>2023-05-26 13:42:41.899304+00:00</td>\n",
       "      <td>3.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://sitewebbixi.s3.amazonaws.com/uploads/d...</td>\n",
       "      <td>biximontrealrentals2017-d4d086.zip</td>\n",
       "      <td>2023-05-26 13:42:45.069033+00:00</td>\n",
       "      <td>4.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://sitewebbixi.s3.amazonaws.com/uploads/d...</td>\n",
       "      <td>biximontrealrentals2018-96034e.zip</td>\n",
       "      <td>2023-05-26 13:42:49.305501+00:00</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       extracted_url  \\\n",
       "0  https://sitewebbixi.s3.amazonaws.com/uploads/d...   \n",
       "1  https://sitewebbixi.s3.amazonaws.com/uploads/d...   \n",
       "2  https://sitewebbixi.s3.amazonaws.com/uploads/d...   \n",
       "3  https://sitewebbixi.s3.amazonaws.com/uploads/d...   \n",
       "4  https://sitewebbixi.s3.amazonaws.com/uploads/d...   \n",
       "\n",
       "                            file_name                    date_accessed  \\\n",
       "0  biximontrealrentals2014-f040e0.zip 2023-05-26 13:42:35.764567+00:00   \n",
       "1  biximontrealrentals2015-69fdf0.zip 2023-05-26 13:42:38.769303+00:00   \n",
       "2  biximontrealrentals2016-912f00.zip 2023-05-26 13:42:41.899304+00:00   \n",
       "3  biximontrealrentals2017-d4d086.zip 2023-05-26 13:42:45.069033+00:00   \n",
       "4  biximontrealrentals2018-96034e.zip 2023-05-26 13:42:49.305501+00:00   \n",
       "\n",
       "   time_taken_seconds  \n",
       "0               3.099  \n",
       "1               2.961  \n",
       "2               3.296  \n",
       "3               4.206  \n",
       "4               4.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add date accessed and time taken to url_df\n",
    "url_df['date_accessed'] = access_date_list\n",
    "url_df['time_taken_seconds'] = time_taken_list\n",
    "\n",
    "# Visually examine first 5 rows of url_df\n",
    "print(f\"The average download time is {np.round(url_df['time_taken_seconds'].mean(),3)} seconds.\")\n",
    "display(url_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally confirm that the downloaded files are in the local system using the os module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # to examine local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
=======
   "execution_count": 65,
>>>>>>> parent of af2591a (added some of part 2)
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hc/dd4qvphd7j59ktfp1751r_4h0000gn/T/ipykernel_3620/917607373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoupified_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cf' is not defined"
     ]
    }
   ],
   "source": [
    "soupified_data = cf.convert_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_link() missing 2 required positional arguments: 'url_list' and 'search_term'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hc/dd4qvphd7j59ktfp1751r_4h0000gn/T/ipykernel_2035/335071804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoupified_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: extract_link() missing 2 required positional arguments: 'url_list' and 'search_term'"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "Now that the data files are downloaded onto the local system, the next step is to unzip the data files and combine the decompressed data into a single data file. In other cases, the decompressed data would be further cleaned and processed as part of an extract, transform, and load (ETL) pipeline before being stored in a database or used for analysis, but for this notebook we will end with a single comma separated value file containing all the data.\n",
    "\n",
    "To unzip files, the `zipfile` package will be used, documentation [here](https://docs.python.org/3/library/zipfile.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3451111/unzipping-files-in-python\n",
    "for file in os.listdir('data/bixi/'):\n",
    "    path_to_zip_file = 'data/bixi/' + file\n",
    "    directory_to_extract_to = 'data/'\n",
    "    \n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(directory_to_extract_to)"
=======
    "links = cf.extract_link()"
>>>>>>> parent of af2591a (added some of part 2)
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "streamlit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
