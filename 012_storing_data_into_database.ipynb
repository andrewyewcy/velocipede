{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e508d32c-6fa7-4035-85cd-b36672882e34",
   "metadata": {},
   "source": [
    "![003](assets/images/013/003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fad2107-2ae9-4948-b92c-3cc9f7286d8c",
   "metadata": {},
   "source": [
    "[Last time](https://andrewyewcy.com/MySQL-and-phpMyAdmin-on-Docker/), MySQL and phpMyAdmin were setup using Docker containers to store the bicycle rides data from Bixi. In this article, we explore how to Extract, Transform, and Load(ETL) the raw data into the MySQL database using SQLAlchemy in Python. XXX rides loaded "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059ebc7-16aa-4b7b-a9bd-b196a45ca5b7",
   "metadata": {},
   "source": [
    "# Motivation and Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e61f12-7681-4de3-8f8b-4c9ef9ac88f4",
   "metadata": {},
   "source": [
    "In the age of Big Data, it is unlikely that the required data is stored in a single MySQL database or within a single DBMS. As such, rather than using a tool specific to MySQL for data ingestion, Python was chosen as the central tool to process raw data for data ingestion into the MySQL database. This avoids the need of using various GUIs to manage data ingestion into different DBMS, and also consolidates the process into a central location for easier maintenance and improvement. \n",
    "\n",
    "Below are some examples of where Python can be used for data ingestion:\n",
    "- **SQL Databases** using [`SQLAlchemy`](https://www.sqlalchemy.org/), a Python toolkit that allows data scientists to connect to many DBMS aside from MySQL such as PostgreSQL and SQLite, all within a single Python file.\n",
    "- **noSQL Databases** like `MongoDB`, `Redis` and `ScyllaDB`, which are tools more familiar to software developers. These do not need to follow the structured relational table format like SQL databases.\n",
    "- **serverless databases on cloud** like `Amazon Aurora`\n",
    "- **stream data** like that using `Apache Kafka`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03265269-5a09-419e-9b88-be6507e802a7",
   "metadata": {},
   "source": [
    "Using Python also allows the use of the Pandas library, which simplifies the code for performing ETL and chunking the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144cbc3b-64f2-43b8-a700-483bd8f7ec57",
   "metadata": {},
   "source": [
    "# Overview of Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177cb36b-eecf-4e67-9415-1c88f94bbcac",
   "metadata": {},
   "source": [
    "## Enviroments using Docker-compose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11fb913-9b6f-4fe2-9919-938cdfbfe051",
   "metadata": {},
   "source": [
    "The environments needed can be easily replicated on any computer with Docker install using this [Docker compose file](https://github.com/andrewyewcy/docker/blob/main/setup.yaml).\n",
    "\n",
    "Place the Docker compose YAML file into your working directory, then run the below line in Terminal to setup the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b3326-8141-4204-8d0f-8f64cc2b2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run below line in Terminal, with your current directory in Terminal same as your chosen working directory\n",
    "docker-compose -f setup.yaml up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee10f5d-78a8-4b5b-bb83-897247e83b0c",
   "metadata": {},
   "source": [
    "3 Docker containers and a Docker network connecting them all will be automatically setup:\n",
    "- Container1: MySQL on port 3306\n",
    "- Container2: phpMyAdmin on port 8080\n",
    "- Container3: `jupyter/pyspark-notebook` on port 10000, containing most Python data science pacakges and Apache Spark for dealing with big data later. [details](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#jupyter-pyspark-notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88ada7-742c-4410-8bc0-f40d80c3461b",
   "metadata": {},
   "source": [
    "Once the containers are up and running, access JupyterLab by going to `localhost:10000` on a browser of your choice and inputting the access token, which can be found in the same terminal shown in blue below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98301aa9-996c-4484-8b39-b2fe20234a14",
   "metadata": {},
   "source": [
    "![004](assets/images/013/004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c960ce-5751-420c-95e4-6d3d7459e975",
   "metadata": {},
   "source": [
    "The contents of your working directory will be within the `work` folder shown below after accessing JupyterLab, and any changes made will be preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af3f7d4-d9b0-47a6-aec1-e4d4a94aa12c",
   "metadata": {},
   "source": [
    "![005](assets/images/013/005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4fd2b-39a7-4c57-b279-3988f5d20814",
   "metadata": {},
   "source": [
    "## `mysqlclient` for SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f7b8c-9ee4-465d-b99e-8824b6a7f541",
   "metadata": {},
   "source": [
    "No additional packages need to be installed except for `mysqlclient`, which is needed by SQLAlchemy to connect Python to MySQL. This is done using the below steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b27df-b0d7-4fd3-97c4-1f169fc3d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Opening a new terminal window, check the name of the Python container\n",
    "docker ps\n",
    "\n",
    "# 2. activate an interactive terminal within the Python container\n",
    "docker exec -it {name_of_python_container} /bin/bash\n",
    "\n",
    "# 3. use conda to install mysqlclient\n",
    "conda install -c conda-forge mysqlclient\n",
    "\n",
    "# 4. when done, exit the interactive terminal within the Docker container\n",
    "exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62e25e-444d-4824-8531-0430d918c830",
   "metadata": {},
   "source": [
    "## Import Packages and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9054bc73-4d45-42a8-8380-ec0073a1e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827732d-60f5-4480-add4-8c8fdfd15c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_list(list):\n",
    "    '''\n",
    "    This functions takes a list and iterates over it to print out the contents of the list.\n",
    "\n",
    "    Inputs\n",
    "    ---\n",
    "    list: a Python list\n",
    "\n",
    "    Outputs\n",
    "    ---\n",
    "    printout of list content with index\n",
    "    '''\n",
    "    \n",
    "    if len(list) == 0:\n",
    "        print(f\"Passed list is empty.\")\n",
    "    else:\n",
    "        for index, item in enumerate(list):\n",
    "            print(f\"Item {index + 1} / {len(list)}: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "962ac1ba-3bc2-4d29-9a50-0e416faf8ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_folder(zip_file_list):\n",
    "    '''\n",
    "    This function takes a list of ZIP files and iterates over each ZIP file to decompress the contents into a folder of the same name\n",
    "    The ZIP data must be placed within the 'data' folder of your working directory\n",
    "\n",
    "    Inputs\n",
    "    ---\n",
    "    zip_file_list: a list of zip file names\n",
    "\n",
    "    Outputs\n",
    "    ---\n",
    "    printout of current decompression progress\n",
    "    zip_df: a DataFrame containing the ZIP file and its contents\n",
    "    '''\n",
    "    \n",
    "    # Initiate blank DataFrame to store log details about each unzipped file\n",
    "    zip_df = pd.DataFrame(\n",
    "        columns = ['zip_file', 'contents']\n",
    "    )\n",
    "    \n",
    "    # Iterating over each zip file\n",
    "    for index, zip_file in enumerate(zip_file_list):\n",
    "        \n",
    "        # Define path to each zip file\n",
    "        path_to_zip_file = 'data/' + zip_file\n",
    "        \n",
    "        # Define directorty to dump all extracted zip files\n",
    "        directory_to_extract_to = 'data/' + zip_file[:-4]\n",
    "\n",
    "        # Create above directory\n",
    "        try:\n",
    "            os.mkdir(directory_to_extract_to)\n",
    "        except OSError as error:\n",
    "            print(error)    \n",
    "        \n",
    "        # With each zipfile opened as a zipfile object\n",
    "        with zipfile.ZipFile(path_to_zip_file,'r') as zip_ref:\n",
    "            \n",
    "            # Create a temporary DataFrame to store log information about zipfiles\n",
    "            temp_df = pd.DataFrame(columns = ['zip_file','contents'])\n",
    "            \n",
    "            # Gather the contents within each zipfile\n",
    "            temp_df['contents'] = zip_ref.namelist()\n",
    "            \n",
    "            # Label from which zipfile were the contents extracted from\n",
    "            temp_df['zip_file'] = zip_file\n",
    "            \n",
    "            # Concatenate the log for specific opened zipfile with rest of logs\n",
    "            zip_df = pd.concat([zip_df, temp_df]).reset_index(drop = True)\n",
    "            \n",
    "            # Extract all contents out of zipfile into specified directory\n",
    "            zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "        print(f\"Unzipped file {index + 1} of {len(zip_file_list)}.\", end = '\\r')\n",
    "        \n",
    "    return zip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d6585607-7dd7-4663-af69-a6e71cb8af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_subfolders(folder_list):\n",
    "    '''\n",
    "    This function takes a list of folders and iterates over each folder to flatten any subfolders within it, then removes the empty subfolders\n",
    "\n",
    "    Inputs\n",
    "    ---\n",
    "    folder_list: a list of folders containing subfolders\n",
    "\n",
    "    Outputs\n",
    "    ---\n",
    "    printout of current flattening progress\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    for index, folder in enumerate(folder_list):\n",
    "\n",
    "        # Define main folder\n",
    "        main_folder = 'data/' + folder\n",
    "    \n",
    "        # Identify all subfolders within the main folder\n",
    "        subfolders = [folder.path for folder in os.scandir(main_folder) if folder.is_dir()]\n",
    "    \n",
    "        # Iterating through each subfolder\n",
    "        for subfolder in subfolders:\n",
    "    \n",
    "            # Iterating through each file within each subfolder\n",
    "            for file in os.listdir(subfolder):\n",
    "                \n",
    "                # Define origin filepath, i.e. the file within the subfolder to be moved\n",
    "                origin = os.path.join(subfolder, file)\n",
    "                \n",
    "                # Define destination filepath, i.e. the main folder with all the other data\n",
    "                destination = os.path.join(main_folder, file)\n",
    "                \n",
    "                # Move file from origin within subfolder out to main folder\n",
    "                shutil.move(origin,destination)\n",
    "    \n",
    "            # Remove subfolder after all files have been moved\n",
    "            shutil.rmtree(subfolder)\n",
    "    \n",
    "        print(f\"Flattened subfolder {index + 1} of {len(folder_list)}.\", end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344314e8-65e9-42fc-9ecf-e9043b0d133d",
   "metadata": {},
   "source": [
    "# Decompress the Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df4c15-d257-4d3d-93d0-1c60190dace2",
   "metadata": {},
   "source": [
    "Before making any decisions in database architecture and design, it is imperitive to understand what the data will be used for. For this case, the Bixi bicycle ride data will be: \n",
    "-  combined with rides data from other companies to generate an analytics dashboard to monitor operations and membership program\n",
    "-  used for perform time series analysis and machine learning to possibly predict bicycle needs across stations and times of day for better bicyle distribution\n",
    "\n",
    "[Previously](https://andrewyewcy.com/Systematically-Web-Scrape-Multiple-Data-Files-from-Websites/), the rides data were web-scraped off from the Bixi website and stored as unpacked ZIP files in a single folder. A problem was that the granularity of data across years for station data was not consistent, causing difficulty in assigning station data to rides data. Thus, to remedy this, we will be starting from the ZIP files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "20da046a-dcd8-4162-8c70-e3c013646b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1 / 16: biximontrealrentals2014-f040e0.zip\n",
      "Item 2 / 16: biximontrealrentals2015-69fdf0.zip\n",
      "Item 3 / 16: biximontrealrentals2016-912f00.zip\n",
      "Item 4 / 16: biximontrealrentals2017-d4d086.zip\n",
      "Item 5 / 16: biximontrealrentals2018-96034e.zip\n",
      "Item 6 / 16: biximontrealrentals2019-33ea73.zip\n",
      "Item 7 / 16: biximontrealrentals2020-8e67d9.zip\n",
      "Item 8 / 16: 2021-donnees-ouvertes-464ae6.zip\n",
      "Item 9 / 16: 20220104-stations-f82036.zip\n",
      "Item 10 / 16: 20220105-donnees-ouvertes-0d544b.zip\n",
      "Item 11 / 16: 20220106-donnees-ouvertes-f45195.zip\n",
      "Item 12 / 16: 20220107-donnees-ouvertes-8aa623.zip\n",
      "Item 13 / 16: 20220108-donnees-ouvertes-816bd4.zip\n",
      "Item 14 / 16: 20220109-donnees-ouvertes-519d43.zip\n",
      "Item 15 / 16: 20220110-donnees-ouvertes-5079e8.zip\n",
      "Item 16 / 16: 20220111-donnees-ouvertes-e1c737.zip\n"
     ]
    }
   ],
   "source": [
    "# Use web-scrape log files to identify the zip files previously web-scrapped\n",
    "log_df = pd.read_csv('logs/log_df.csv')\n",
    "\n",
    "# Visually examine the zip folders\n",
    "zip_file_list = log_df.loc[:,'file_name'].to_list()\n",
    "print_list(zip_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbb0c6-dbd3-4efb-8495-6e6337059a5a",
   "metadata": {},
   "source": [
    "The bicycle rides data were contained in 16 ZIP files, with the one file for each year except for the most recent 2022. To access the contents within each file, the defined `unzip_folder` function was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a116860c-6068-436a-98ff-44ab82db3e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped file 16 of 16\r"
     ]
    }
   ],
   "source": [
    "# Unzip all 16 folders\n",
    "zip_df = unzip_folder(zip_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804188a-a029-4d60-9a36-4999e791c7c1",
   "metadata": {},
   "source": [
    "Then all subfolders within each unzipped rides data folder were flattened to access the files easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "bfd2bc5e-e1a2-4eca-92f2-85901593c9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened subfolder 16 of 16.\r"
     ]
    }
   ],
   "source": [
    "# Define list of folders to perform flattening for: exclude ZIP files and hidden files\n",
    "folder_list = [folder for folder in os.listdir('data/') if folder[-4:] != '.zip' and folder[0:1] != '.']\n",
    "\n",
    "# Flatten all subfolders within the 16 unzipped folders\n",
    "flatten_subfolders(folder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ad285bf7-f630-4f45-beef-f24ab4e1cbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1 / 16: 2021-donnees-ouvertes-464ae6\n",
      "Item 2 / 16: 20220104-stations-f82036\n",
      "Item 3 / 16: 20220105-donnees-ouvertes-0d544b\n",
      "Item 4 / 16: 20220106-donnees-ouvertes-f45195\n",
      "Item 5 / 16: 20220107-donnees-ouvertes-8aa623\n",
      "Item 6 / 16: 20220108-donnees-ouvertes-816bd4\n",
      "Item 7 / 16: 20220109-donnees-ouvertes-519d43\n",
      "Item 8 / 16: 20220110-donnees-ouvertes-5079e8\n",
      "Item 9 / 16: 20220111-donnees-ouvertes-e1c737\n",
      "Item 10 / 16: biximontrealrentals2014-f040e0\n",
      "Item 11 / 16: biximontrealrentals2015-69fdf0\n",
      "Item 12 / 16: biximontrealrentals2016-912f00\n",
      "Item 13 / 16: biximontrealrentals2017-d4d086\n",
      "Item 14 / 16: biximontrealrentals2018-96034e\n",
      "Item 15 / 16: biximontrealrentals2019-33ea73\n",
      "Item 16 / 16: biximontrealrentals2020-8e67d9\n"
     ]
    }
   ],
   "source": [
    "# Visually examine flattened folders\n",
    "print_list(folder_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e366fd6-bfa1-4182-ae5b-d854d96801da",
   "metadata": {},
   "source": [
    "# Sample the Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686ff53-07cf-4693-ba8d-a07e14b9039f",
   "metadata": {},
   "source": [
    "Although data sampling can be done using Pandas in Python, Terminal and Bash were used instead to perform this sanity check. Bash with its simplicity is much faster than Pandas in counting rows and displays the data as plain text without any consideration of data types, making it the perfect tool for quickly examining large data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60a1f5-eb9f-40cf-ac7b-083d49c10a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peform row count for the year 2014 using terminal\n",
    "# Run this for loop line by line in terminal\n",
    "for file in *.csv\n",
    "    do\n",
    "    wc -l $file\n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be195ffe-85df-474f-ba88-3f6b83c012cf",
   "metadata": {},
   "source": [
    "![check_lines](assets/images/013/001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796fa5ff-5f41-474d-944d-ca4a52129f27",
   "metadata": {},
   "source": [
    "As seen above, there are roughly 3 million rides in the year 2014, meaning that the whole dataset will be roughly 27 million rows of data detailing the Bixi bicycle rides in Montreal over 9 years. The sheer number of rows means justifies the use of Python and MySQL over traditional spreadsheets as the latter are unable to handle the large data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b533c-af5d-48ca-b0b9-c82115663637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually examine first 10 rows of rides data in the file OD_2014-10.csv within the year 2014\n",
    "less -NS OD_2014-10.csv | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e47fb-175e-4ba0-8202-2addbae86f6b",
   "metadata": {},
   "source": [
    "![002](assets/images/013/002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768cb0b-057d-4ac8-bc83-732afd4c81b6",
   "metadata": {},
   "source": [
    "From above, the rides data contains 6 columns or features:\n",
    "1. `start_date`: the datetime when a Bixi bicycle is checked out from a dock\n",
    "2. `start_station_code`: the docking station where the Bixi bicycles was checked out from\n",
    "3. `end_date`: the datetime when a Bixi bicycle is returned to a dock\n",
    "4. `end_station_code`: the docking station where the checked out bicycle was returned to\n",
    "6. `duration_sec`: the total time of the ride\n",
    "7. `is_member`: the membership status of the customer who used the Bixi bicycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983bc899-b43a-48cd-b996-8f5869912487",
   "metadata": {},
   "source": [
    "![006](assets/images/013/006.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b163e52-fb95-4467-b364-03cedab5cfa5",
   "metadata": {},
   "source": [
    "Repeating the procedure for bicycle docking stations data:\n",
    "1. `code`: the code of the station, used to map station info to rides table\n",
    "2. `name`: the human readable name of the station\n",
    "3. `latitude`: the latitude of the station\n",
    "4. `longitude`: the longitude of the station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1a695-3ee6-49b4-aa85-6f09aa7fc296",
   "metadata": {},
   "source": [
    "# Designing Tables and Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855648cd-2eb8-44ab-b7b9-bdff370b1dd0",
   "metadata": {},
   "source": [
    "Now that we have a better understanding of the data, it is clear that at least two tables need to be created in the database: `rides` and `stations`.\n",
    "\n",
    "The `rides` table will contain the data about each ride, and, is connected to the `stations` table using the `code` column. But, depending on time, the exact longitude and latitude of each docking station may change due to circumstances such as road repairs. This implies that the `stations` data for 2014 may not be applicable in 2015 and other years. In terms of impact, distance per ride calculations will change depending on latitude and longitude, and should be considered when deciding station locations.\n",
    "\n",
    "Furthermore, as mentioned above, unlike all other years which had 1 `stations` table per year, the year 2022 had 1 `stations` table per month, complicating the granularity and thus the method of connecting the `rides` and `stations` tables consistently throughout the years.\n",
    "\n",
    "To solve this issue of granularity, a third table is created, containing the details on how to join the `rides` and `stations` table for each year. This table is called a join table and can be generated by examining the contents of each folder. Each folder contains a file with the word `station` in the file name for the station data while the other files are ride data. Using this pattern a join table can be generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d72c6446-86ce-4747-998d-5759a923be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder 16 of 16, folder: biximontrealrentals2020-8e67d97\r"
     ]
    }
   ],
   "source": [
    "join_table = pd.DataFrame()\n",
    "\n",
    "for index, folder in enumerate(folder_list):\n",
    "    \n",
    "    folder_contents = os.listdir('data/'+ folder)\n",
    "\n",
    "    stations_files = [file for file in folder_contents if 'station' in file.lower()]\n",
    "\n",
    "    rides_files = [file for file in folder_contents if file not in stations_files]\n",
    "\n",
    "    temp_df = pd.DataFrame({'rides_files':rides_files})\n",
    "    \n",
    "    if len(stations_files) == 1:\n",
    "        temp_df['station_files'] = stations_files[0]\n",
    "    else:\n",
    "        print(\"Check station file in folder {index + 1} : {folder}.\")\n",
    "\n",
    "    join_table = pd.concat(\n",
    "        [join_table,temp_df],\n",
    "        axis = 0,\n",
    "    ).reset_index(drop = True)\n",
    "\n",
    "    print(f'Processing folder {index+1} of {len(folder_list)}, folder: {folder}', end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e8d9d817-20b1-4243-8228-ba3f6c3861b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rides_files</th>\n",
       "      <th>station_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021_donnees_ouvertes.csv</td>\n",
       "      <td>2021_stations.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20220104_donnees_ouvertes.csv</td>\n",
       "      <td>20220104_stations.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20220105_donnees_ouvertes.csv</td>\n",
       "      <td>20220105_stations.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20220106_donnees_ouvertes.csv</td>\n",
       "      <td>20220106_stations.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20220107_donnees_ouvertes.csv</td>\n",
       "      <td>20220107_stations.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20220108_donnees_ouvertes.csv</td>\n",
       "      <td>20220108_stations.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>202209_deplacements.csv</td>\n",
       "      <td>202209_stations.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>202210_deplacements.csv</td>\n",
       "      <td>202210_stations.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>202211_deplacements.csv</td>\n",
       "      <td>202211_stations.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OD_2014-04.csv</td>\n",
       "      <td>Stations_2014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OD_2014-05.csv</td>\n",
       "      <td>Stations_2014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OD_2014-06.csv</td>\n",
       "      <td>Stations_2014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OD_2014-07.csv</td>\n",
       "      <td>Stations_2014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>OD_2014-08.csv</td>\n",
       "      <td>Stations_2014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OD_2014-09.csv</td>\n",
       "      <td>Stations_2014.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      rides_files          station_files\n",
       "0       2021_donnees_ouvertes.csv      2021_stations.csv\n",
       "1   20220104_donnees_ouvertes.csv  20220104_stations.csv\n",
       "2   20220105_donnees_ouvertes.csv  20220105_stations.csv\n",
       "3   20220106_donnees_ouvertes.csv  20220106_stations.csv\n",
       "4   20220107_donnees_ouvertes.csv  20220107_stations.csv\n",
       "5   20220108_donnees_ouvertes.csv  20220108_stations.csv\n",
       "6         202209_deplacements.csv    202209_stations.csv\n",
       "7         202210_deplacements.csv    202210_stations.csv\n",
       "8         202211_deplacements.csv    202211_stations.csv\n",
       "9                  OD_2014-04.csv      Stations_2014.csv\n",
       "10                 OD_2014-05.csv      Stations_2014.csv\n",
       "11                 OD_2014-06.csv      Stations_2014.csv\n",
       "12                 OD_2014-07.csv      Stations_2014.csv\n",
       "13                 OD_2014-08.csv      Stations_2014.csv\n",
       "14                 OD_2014-09.csv      Stations_2014.csv"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visually examine the join_table\n",
    "join_table.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1df72e-aa2f-468f-bb14-6cd4eb7b2f26",
   "metadata": {},
   "source": [
    "From above, it was observed that each ride file is mapped to each station file, and cases like 2014 where many ride files map to a single station file were also recorded accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2d73037b-85b6-4402-a072-895d4a127ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021_donnees_ouvertes.csv', '2021_stations.csv']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_contents = os.listdir('data/'+ folder_list[0])\n",
    "\n",
    "folder_contents.lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5c6ef-7e75-41d9-b543-946d6a0c18af",
   "metadata": {},
   "source": [
    "## Installing `mysqlclient` on Python Docker Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa142c6-6f0c-4930-80a1-ef10152690c5",
   "metadata": {},
   "source": [
    "The [`SQL Alchemy`](https://www.sqlalchemy.org/) library was used to connect to the MySQL database. [`SQL Alchemy`](https://www.sqlalchemy.org/) allows the use of many different DBMS through Python, and is the prefered connection when using Pandas with SQL.\n",
    "\n",
    "On top of `SQLAlchemy`, [`mysqlclient`](https://docs.sqlalchemy.org/en/20/dialects/mysql.html#module-sqlalchemy.dialects.mysql.mysqldb) is according to [documentation](https://docs.sqlalchemy.org/en/20/dialects/mysql.html#module-sqlalchemy.dialects.mysql.mysqldb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d76ff69-f34d-4b16-bd05-19341237b725",
   "metadata": {},
   "source": [
    "`SQLAlchemy` was already included in the Docker image for [pyspark-notebook](https://hub.docker.com/r/jupyter/pyspark-notebook). `mysqlclient` was added to the container using `conda` within the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07857860-d38a-40c5-8705-acdef1e4b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using terminal, access the terminal within the running pyspark container\n",
    "docker exec -it documents-pyspark-1 /bin/bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ca7fc-c257-4b9d-966a-4633fc0b3530",
   "metadata": {},
   "source": [
    "Explaining the hashes above:\n",
    "- `-it` stands for iteractive terminal\n",
    "- `documents-pyspark-1` is the name of the running Docker container\n",
    "- `/bin/bash` provides a terminal running Bash\n",
    "\n",
    "Then, refering to instructions from `conda` [docs](https://anaconda.org/conda-forge/mysqlclient), `mysqlclient` can be installed in the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b34e574-ff60-4b98-b429-661a9f635655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mysqlclient using conda inside the container\n",
    "conda install -c conda-forge mysqlclient\n",
    "\n",
    "# exit the terminal within the container\n",
    "exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d585b-2376-435e-ac41-a8d008d34c17",
   "metadata": {},
   "source": [
    "## Connect to MySQL Docker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9879fb3-7f20-47db-a318-a75d4244c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create the engine that connects Python and MySQL\n",
    "import sqlalchemy\n",
    "\n",
    "# Create the connection engine\n",
    "engine = sqlalchemy.create_engine(\n",
    "    \"mysql+mysqldb://root:rootroot@mysql:3306\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583ffc5-444c-4b42-b769-0a81e3115763",
   "metadata": {},
   "source": [
    "- `mysql+mysqldb` are the drivers from `mysqlclient`\n",
    "- `root:rootroot` stands for the username:password\n",
    "- `@mysql:3306` refers to port 3306 in the `mysql` Docker container\n",
    "\n",
    "Note that the `with` statement is used when establishing connections to MySQL as it ensures that the connection is automatically closed once the SQL query is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "14eacacd-2f62-4049-a53d-bdd21d22cf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 1 / 5: information_schema\n",
      "Database 2 / 5: mysql\n",
      "Database 3 / 5: performance_schema\n",
      "Database 4 / 5: sys\n",
      "Database 5 / 5: test_db\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/22689895/list-of-databases-in-sqlalchemy\n",
    "# Print existing databases\n",
    "insp = sqlalchemy.inspect(engine)\n",
    "databases = insp.get_schema_names()\n",
    "\n",
    "for index, database in enumerate(databases):\n",
    "    print(f\"Database {index + 1} / {len(databases)}: {database}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c3a15-ad1f-4aea-9c45-b0889929c1cd",
   "metadata": {},
   "source": [
    "## Create A Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "147541db-2a84-439d-b335-735d6d52c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database to store bicycle rides data\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    # Creates a DATABASE if it doesn't exist\n",
    "    sql_stmt = sqlalchemy.text(\"\"\"CREATE DATABASE IF NOT EXISTS velocipede\"\"\")\n",
    "    \n",
    "    conn.execute(sql_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2bdec5c-0d15-4238-90bd-739c1a97d3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b159c50f-ed62-4348-87a8-a7bdd26ebf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 1 / 6: information_schema\n",
      "Database 2 / 6: mysql\n",
      "Database 3 / 6: performance_schema\n",
      "Database 4 / 6: sys\n",
      "Database 5 / 6: test_db\n",
      "Database 6 / 6: velocipede\n"
     ]
    }
   ],
   "source": [
    "# Check if new database was created\n",
    "inspection = sqlalchemy.inspect(engine)\n",
    "databases = inspection.get_schema_names()\n",
    "\n",
    "for index, database in enumerate(databases):\n",
    "    print(f\"Database {index + 1} / {len(databases)}: {database}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08b05b-8f81-47ed-b961-dcf1a81d1340",
   "metadata": {},
   "source": [
    "## Create `rides` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3db90792-7646-4fa6-8449-f1b2c1ee83b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed list is empty.\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/6473925/sqlalchemy-getting-a-list-of-tables\n",
    "# Redefine new engine to point to newly created database\n",
    "engine = sqlalchemy.create_engine(\n",
    "    \"mysql+mysqldb://root:rootroot@mysql:3306/velocipede\"\n",
    ")\n",
    "\n",
    "# Check tables within \n",
    "inspection = sqlalchemy.inspect(engine)\n",
    "tables = inspection.get_table_names()\n",
    "\n",
    "print_list(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bd94c69c-7612-4c4b-b80b-17153045de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    sql_stmt = sqlalchemy.text(\"\"\"DROP TABLE IF EXISTS rides\"\"\")\n",
    "    conn.execute(sql_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a5ef0aae-7d7b-4795-a0d9-eb03800c8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new table\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    sql_stmt = text(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS rides (\n",
    "            # columns regarding rides, assume each ride unique given no ride_id provided\n",
    "            ride_id\t\t\tBIGINT\t\t\tNOT NULL AUTO_INCREMENT,\n",
    "        \tstart_date \t\tDATETIME \t\tNOT NULL,\n",
    "        \tstart_stn_code \tVARCHAR(255)\tNOT NULL,\n",
    "        \tend_date \t\tDATETIME \t\tNOT NULL,\n",
    "        \tend_stn_code \tVARCHAR(255) \tNOT NULL,\n",
    "            duration_sec\tINT \t\t\tNOT NULL,\n",
    "        \tis_member \t\tBOOLEAN,\n",
    "            company\t\t\tVARCHAR(255)\tNOT NULL,\n",
    "            timezone        VARCHAR(255)    NOT NULL,\n",
    "\n",
    "            # columns for data maintenance\n",
    "            data_source\t\tVARCHAR(255)\tNOT NULL,\n",
    "            date_added\t\tDATETIME \t\tNOT NULL DEFAULT CURRENT_TIMESTAMP,\n",
    "            flag\t\t\tVARCHAR(255),\n",
    "\n",
    "            # Each ride is unique\n",
    "            CONSTRAINT ride_uid UNIQUE (ride_id)\n",
    "            )    \n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    conn.execute(sql_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fa5cd9e6-07ce-461e-abbf-dd042752988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1 / 2: rides\n",
      "Item 2 / 2: stations\n"
     ]
    }
   ],
   "source": [
    "# Check tables within \n",
    "inspection = sqlalchemy.inspect(engine)\n",
    "tables = inspection.get_table_names()\n",
    "\n",
    "print_list(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "79d319a9-7527-4ce2-9815-06f5315c79c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1 / 12: {'name': 'ride_id', 'type': BIGINT(), 'default': None, 'comment': None, 'nullable': False, 'autoincrement': True}\n",
      "Item 2 / 12: {'name': 'start_date', 'type': DATETIME(), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 3 / 12: {'name': 'start_stn_code', 'type': VARCHAR(length=255), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 4 / 12: {'name': 'end_date', 'type': DATETIME(), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 5 / 12: {'name': 'end_stn_code', 'type': VARCHAR(length=255), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 6 / 12: {'name': 'duration_sec', 'type': INTEGER(), 'default': None, 'comment': None, 'nullable': False, 'autoincrement': False}\n",
      "Item 7 / 12: {'name': 'is_member', 'type': TINYINT(display_width=1), 'default': None, 'comment': None, 'nullable': True, 'autoincrement': False}\n",
      "Item 8 / 12: {'name': 'company', 'type': VARCHAR(length=255), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 9 / 12: {'name': 'timezone', 'type': VARCHAR(length=255), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 10 / 12: {'name': 'data_source', 'type': VARCHAR(length=255), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 11 / 12: {'name': 'date_added', 'type': DATETIME(), 'default': 'CURRENT_TIMESTAMP', 'comment': None, 'nullable': False}\n",
      "Item 12 / 12: {'name': 'flag', 'type': VARCHAR(length=255), 'default': None, 'comment': None, 'nullable': True}\n"
     ]
    }
   ],
   "source": [
    "columns = inspection.get_columns('rides')\n",
    "\n",
    "print_list(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0b5b3-c88c-4dac-ad78-6200299b8271",
   "metadata": {},
   "source": [
    "## Create `stations` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5528e904-ee5b-432f-bd4b-ec84943c131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new table\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    sql_stmt = text(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS stations (\n",
    "            # columns regarding rides, assume each ride unique given no ride_id provided\n",
    "            stn_id\t\t\tBIGINT\t\t\tNOT NULL AUTO_INCREMENT,\n",
    "        \tstn_code \t    VARCHAR(255)\tNOT NULL,\n",
    "            stn_name        VARCHAR(255)\tNOT NULL,\n",
    "            stn_lat         DECIMAL(7,5)    NOT NULL,\n",
    "            stn_lon         DECIMAL(7,5)    NOT NULL,\n",
    "            company\t\t\tVARCHAR(255)\tNOT NULL,\n",
    "\n",
    "            # columns for data maintenance\n",
    "            data_source\t\tVARCHAR(255)\tNOT NULL,\n",
    "            date_added\t\tDATETIME \t\tNOT NULL DEFAULT CURRENT_TIMESTAMP,\n",
    "            flag\t\t\tVARCHAR(255),\n",
    "\n",
    "            # Each ride is unique\n",
    "            CONSTRAINT stn_uid UNIQUE (stn_id)\n",
    "            )    \n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    conn.execute(sql_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "629de435-e102-4b13-94d0-a0da1302a16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1 / 2: rides\n",
      "Item 2 / 2: stations\n"
     ]
    }
   ],
   "source": [
    "# Check tables within \n",
    "inspection = sqlalchemy.inspect(engine)\n",
    "tables = inspection.get_table_names()\n",
    "\n",
    "print_list(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "76f3958d-535c-4374-a663-d29297a8d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1 / 9: {'name': 'stn_id', 'type': BIGINT(), 'default': None, 'comment': None, 'nullable': False, 'autoincrement': True}\n",
      "Item 2 / 9: {'name': 'stn_code', 'type': VARCHAR(length=255), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 3 / 9: {'name': 'stn_name', 'type': VARCHAR(length=255), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 4 / 9: {'name': 'stn_lat', 'type': DECIMAL(precision=7, scale=5), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 5 / 9: {'name': 'stn_lon', 'type': DECIMAL(precision=7, scale=5), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 6 / 9: {'name': 'company', 'type': VARCHAR(length=255), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 7 / 9: {'name': 'data_source', 'type': VARCHAR(length=255), 'default': None, 'comment': None, 'nullable': False}\n",
      "Item 8 / 9: {'name': 'date_added', 'type': DATETIME(), 'default': 'CURRENT_TIMESTAMP', 'comment': None, 'nullable': False}\n",
      "Item 9 / 9: {'name': 'flag', 'type': VARCHAR(length=255), 'default': None, 'comment': None, 'nullable': True}\n"
     ]
    }
   ],
   "source": [
    "columns = inspection.get_columns('stations')\n",
    "\n",
    "print_list(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941a7d2-2442-44a4-b96d-6bff523fcc0c",
   "metadata": {},
   "source": [
    "# Populate the Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b3c9662b-7876-4891-87fc-cbe5bf2dbd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'data/biximontrealrentals2014-f040e0/OD_2014-04.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "21425380-5927-41c1-92ef-45919002e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/25962114/how-do-i-read-a-large-csv-file-with-pandas\n",
    "df = pd.read_csv('data/biximontrealrentals2014-f040e0/OD_2014-04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8a095b7e-6c19-47a5-b65c-5402666e8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    'start_station_code' : 'string',\n",
    "    'end_station_code'   : 'string',\n",
    "    'duration_sec'       : 'int',\n",
    "    'is_member'          : 'boolean'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a8c72808-adb7-43b1-a2b7-1d802aa2e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    data_file,\n",
    "    dtype = dtype,\n",
    "    parse_dates = ['start_date','end_date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5d10fc05-a6c0-40c9-b5c9-6c878cd76d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>start_station_code</th>\n",
       "      <th>end_date</th>\n",
       "      <th>end_station_code</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>is_member</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-04-15 00:01</td>\n",
       "      <td>6209</td>\n",
       "      <td>2014-04-15 00:18</td>\n",
       "      <td>6436</td>\n",
       "      <td>1061</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-04-15 00:01</td>\n",
       "      <td>6214</td>\n",
       "      <td>2014-04-15 00:11</td>\n",
       "      <td>6248</td>\n",
       "      <td>615</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-04-15 00:01</td>\n",
       "      <td>6164</td>\n",
       "      <td>2014-04-15 00:18</td>\n",
       "      <td>6216</td>\n",
       "      <td>1031</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-04-15 00:01</td>\n",
       "      <td>6214</td>\n",
       "      <td>2014-04-15 00:24</td>\n",
       "      <td>6082</td>\n",
       "      <td>1382</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-04-15 00:02</td>\n",
       "      <td>6149</td>\n",
       "      <td>2014-04-15 00:08</td>\n",
       "      <td>6265</td>\n",
       "      <td>347</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         start_date start_station_code          end_date end_station_code  \\\n",
       "0  2014-04-15 00:01               6209  2014-04-15 00:18             6436   \n",
       "1  2014-04-15 00:01               6214  2014-04-15 00:11             6248   \n",
       "2  2014-04-15 00:01               6164  2014-04-15 00:18             6216   \n",
       "3  2014-04-15 00:01               6214  2014-04-15 00:24             6082   \n",
       "4  2014-04-15 00:02               6149  2014-04-15 00:08             6265   \n",
       "\n",
       "   duration_sec  is_member  \n",
       "0          1061       True  \n",
       "1           615       True  \n",
       "2          1031       True  \n",
       "3          1382       True  \n",
       "4           347       True  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "29e222aa-2c30-41f0-8d39-9ce86d3c0c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2014-04-15 00:01:00-04:00\n",
       "1        2014-04-15 00:01:00-04:00\n",
       "2        2014-04-15 00:01:00-04:00\n",
       "3        2014-04-15 00:01:00-04:00\n",
       "4        2014-04-15 00:02:00-04:00\n",
       "                    ...           \n",
       "108259   2014-04-30 23:53:00-04:00\n",
       "108260   2014-04-30 23:54:00-04:00\n",
       "108261   2014-04-30 23:55:00-04:00\n",
       "108262   2014-04-30 23:58:00-04:00\n",
       "108263   2014-04-30 23:59:00-04:00\n",
       "Name: start_date, Length: 108264, dtype: datetime64[ns, Canada/Eastern]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz = pytz.timezone('Canada/Eastern')\n",
    "dti = pd.to_datetime(df['start_date'], format = \"%Y-%m-%d %H:%M\").dt.tz_localize(tz = tz)\n",
    "\n",
    "dti#.tz_localize(pytz.timezone('EST5EDT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0f9320cc-50d3-4abe-a5cf-d5689868f8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2014-04-15 04:01:00+00:00\n",
       "1        2014-04-15 04:01:00+00:00\n",
       "2        2014-04-15 04:01:00+00:00\n",
       "3        2014-04-15 04:01:00+00:00\n",
       "4        2014-04-15 04:02:00+00:00\n",
       "                    ...           \n",
       "108259   2014-05-01 03:53:00+00:00\n",
       "108260   2014-05-01 03:54:00+00:00\n",
       "108261   2014-05-01 03:55:00+00:00\n",
       "108262   2014-05-01 03:58:00+00:00\n",
       "108263   2014-05-01 03:59:00+00:00\n",
       "Name: start_date, Length: 108264, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dti = dti.dt.tz_convert(tz = pytz.timezone('UTC'))\n",
    "\n",
    "dti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "20d09091-6865-45e3-be40-a8907c8602bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2014-04-15 04:01:00\n",
       "1        2014-04-15 04:01:00\n",
       "2        2014-04-15 04:01:00\n",
       "3        2014-04-15 04:01:00\n",
       "4        2014-04-15 04:02:00\n",
       "                 ...        \n",
       "108259   2014-05-01 03:53:00\n",
       "108260   2014-05-01 03:54:00\n",
       "108261   2014-05-01 03:55:00\n",
       "108262   2014-05-01 03:58:00\n",
       "108263   2014-05-01 03:59:00\n",
       "Name: start_date, Length: 108264, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dti.dt.tz_convert(tz = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "10f5f048-fea0-48da-a7d5-a72ea2b3e9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Etc/UTC', 'UTC']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[zone for zone in pytz.all_timezones if 'UTC' in zone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4e34b506-5143-46ca-8d94-b78d7e2e8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "05daba75-30f5-4661-9d55-1931c496eaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 11\r"
     ]
    }
   ],
   "source": [
    "# Define data types\n",
    "dtype = {\n",
    "    'start_date'         : 'string',\n",
    "    'start_station_code' : 'string',\n",
    "    'end_date'           : 'string',\n",
    "    'end_station_code'   : 'string',\n",
    "    'duration_sec'       : 'int',\n",
    "    'is_member'          : 'boolean'\n",
    "}\n",
    "\n",
    "# Define list of columns\n",
    "columns = [\n",
    "    'start_date',\n",
    "    'start_station_code',\n",
    "    'end_date',\n",
    "    'end_station_code',\n",
    "    'duration_sec',\n",
    "    'is_member'\n",
    "]\n",
    "\n",
    "montreal_tz = pytz.timezone('Canada/Eastern')\n",
    "utc_tz = pytz.timezone('UTC')\n",
    "\n",
    "for index, chunk in enumerate(\n",
    "    pd.read_csv(\n",
    "        data_file,\n",
    "        dtype = dtype,\n",
    "        usecols = columns,\n",
    "        chunksize = 10000\n",
    "    )\n",
    "):\n",
    "    \n",
    "    chunk.rename(\n",
    "        {\n",
    "            'start_station_code':'start_stn_code',\n",
    "            'end_station_code':'end_stn_code'\n",
    "        },\n",
    "        axis = 'columns',\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    for col in ['start_date', 'end_date']:\n",
    "        dti = pd.to_datetime(chunk[col], format = \"%Y-%m-%d %H:%M\")\n",
    "        # Add to Montreal timezone\n",
    "        dti = dti.dt.tz_localize(tz = montreal_tz)\n",
    "        dti = dti.dt.tz_convert(tz = utc_tz)\n",
    "        chunk.loc[:,col] = dti.dt.tz_convert(tz = None)\n",
    "\n",
    "    chunk['company'] = 'Bixi'\n",
    "    chunk['timezone'] = 'Canada/Eastern'\n",
    "    chunk['data_source'] = 'OD_2014-04.csv'\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        chunk.to_sql(\n",
    "            'rides',\n",
    "            con = conn,\n",
    "            schema = 'velocipede',\n",
    "            if_exists = 'append',\n",
    "            index = False\n",
    "        )\n",
    "\n",
    "    print(f\"Completed chunk {index + 1}\", end = \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "04284fd0-8613-454d-b185-6971f42a7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql(\"SELECT COUNT(*) FROM rides LIMIT 5\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a79e1765-e909-46b4-9579-8014a80530ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNT(*)\n",
       "0    108264"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b1ffd-c412-45af-9f47-9ea9e687ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, chunk in enumerate(\n",
    "    pd.read_csv(\n",
    "        data_file,\n",
    "        dtype = dtype,\n",
    "        usecols = columns\n",
    "    )\n",
    "):\n",
    "\n",
    "    print(chunk.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b669da12-b56f-46e3-a818-a08c41ca6041",
   "metadata": {},
   "source": [
    "## Joining the two tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "373135df-f742-46d9-871d-bf39c47f10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    query = \"\"\"SELECT * FROM velocipede.bixi\"\"\"\n",
    "    df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e686d7d-2165-4808-8499-d383b835d880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_station_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_station_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration_sec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_member</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_added</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [start_date, start_station_code, end_date, end_station_code, duration_sec, is_member, data_source, data_added]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee05cd-241b-474a-abc2-191cb6ee8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS rides (\n",
    "\tstart_date \t\tDATETIME \tNOT NULL,\n",
    "\tstart_stn_code \tTEXT \t\tNOT NULL,\n",
    "\tend_date \t\tDATETIME \tNOT NULL,\n",
    "\tend_stn_code \tTEXT \t\tNOT NULL,\n",
    "    duration_sec\tINT \t\tNOT NULL,\n",
    "\tis_member \t\tBOOLEAN,\n",
    "    data_source\t\tTEXT\t\tNOT NULL,\n",
    "    date_added\t\tDATETIME \tNOT NULL DEFAULT CURRENT_TIMESTAMP,\n",
    "    company\t\t\tTEXT\t\tNOT NULL\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
